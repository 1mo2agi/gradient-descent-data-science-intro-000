{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving Regression Lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section we saw how after choosing the slope and y-intercept values of a regression line, we use the residual sum of squares (RSS) to distill the goodness of fit into one number.  \n",
    "\n",
    "Now we can go beyond that to find the \"best fit\" regression line by doing the following:\n",
    "* Choose a regression line with a guess of values for $m$ and $b$\n",
    "* Calculate the RSS\n",
    "* Adjust $b$ and $m$, as these are the only things that can vary in a single-variable regression line.\n",
    "* Again calculate the RSS \n",
    "* Repeat this process\n",
    "\n",
    "The regression line (that is, the values of $b$ and $m$) with our smallest RSS is our **best fit line**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing goodness of fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see this technique in action.  For this example, let's imagine that our data looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_show = {'x': 100, 'y': 275}\n",
    "second_show = {'x': 200, 'y': 300}\n",
    "third_show = {'x': 400, 'y': 700}\n",
    "\n",
    "shows = [first_show, second_show, third_show]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again use our `build_regression_line` function.  Remember that, the function just takes an initial guess at the slope by drawing a line between the first and last points.  Here, giving us a slope of 1.833.  And from there it calculates the value of $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4166666666666667"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from linear_equations import build_regression_line\n",
    "build_regression_line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def regression_formula(x):\n",
    "    return 1.417*x + 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot this regression formula with our data to get a sense of what this looks like.  First import the necessary libraries to allow us to use `plotly` in our notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use some [custom built functions](https://github.com/learn-co-curriculum/plotly-helpers/blob/master/graph.py) to plot this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "mode": "markers",
         "x": [
          100,
          200,
          400
         ],
         "y": [
          275,
          300,
          700
         ]
        },
        {
         "mode": "line",
         "x": [
          100,
          200,
          400
         ],
         "y": [
          241.70000000000002,
          383.40000000000003,
          666.8000000000001
         ]
        },
        {
         "marker": {
          "color": "red"
         },
         "mode": "lines+text",
         "name": "error at 100",
         "text": [
          "33.3"
         ],
         "textposition": "right",
         "x": [
          100,
          100
         ],
         "y": [
          275,
          241.70000000000002
         ]
        },
        {
         "marker": {
          "color": "red"
         },
         "mode": "lines+text",
         "name": "error at 200",
         "text": [
          "-83.4"
         ],
         "textposition": "right",
         "x": [
          200,
          200
         ],
         "y": [
          300,
          383.40000000000003
         ]
        },
        {
         "marker": {
          "color": "red"
         },
         "mode": "lines+text",
         "name": "error at 400",
         "text": [
          "33.2"
         ],
         "textposition": "right",
         "x": [
          400,
          400
         ],
         "y": [
          700,
          666.8000000000001
         ]
        }
       ],
       "layout": {}
      },
      "text/html": [
       "<div id=\"1135ed01-bb0f-47c5-b755-4ab88b48f220\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"1135ed01-bb0f-47c5-b755-4ab88b48f220\", [{\"x\": [100, 200, 400], \"y\": [275, 300, 700], \"mode\": \"markers\"}, {\"x\": [100, 200, 400], \"y\": [241.70000000000002, 383.40000000000003, 666.8000000000001], \"mode\": \"line\"}, {\"x\": [100, 100], \"y\": [275, 241.70000000000002], \"mode\": \"lines+text\", \"marker\": {\"color\": \"red\"}, \"name\": \"error at 100\", \"text\": [\"33.3\"], \"textposition\": \"right\"}, {\"x\": [200, 200], \"y\": [300, 383.40000000000003], \"mode\": \"lines+text\", \"marker\": {\"color\": \"red\"}, \"name\": \"error at 200\", \"text\": [\"-83.4\"], \"textposition\": \"right\"}, {\"x\": [400, 400], \"y\": [700, 666.8000000000001], \"mode\": \"lines+text\", \"marker\": {\"color\": \"red\"}, \"name\": \"error at 400\", \"text\": [\"33.2\"], \"textposition\": \"right\"}], {}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"1135ed01-bb0f-47c5-b755-4ab88b48f220\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"1135ed01-bb0f-47c5-b755-4ab88b48f220\", [{\"x\": [100, 200, 400], \"y\": [275, 300, 700], \"mode\": \"markers\"}, {\"x\": [100, 200, 400], \"y\": [241.70000000000002, 383.40000000000003, 666.8000000000001], \"mode\": \"line\"}, {\"x\": [100, 100], \"y\": [275, 241.70000000000002], \"mode\": \"lines+text\", \"marker\": {\"color\": \"red\"}, \"name\": \"error at 100\", \"text\": [\"33.3\"], \"textposition\": \"right\"}, {\"x\": [200, 200], \"y\": [300, 383.40000000000003], \"mode\": \"lines+text\", \"marker\": {\"color\": \"red\"}, \"name\": \"error at 200\", \"text\": [\"-83.4\"], \"textposition\": \"right\"}, {\"x\": [400, 400], \"y\": [700, 666.8000000000001], \"mode\": \"lines+text\", \"marker\": {\"color\": \"red\"}, \"name\": \"error at 400\", \"text\": [\"33.2\"], \"textposition\": \"right\"}], {}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from graph import trace, plot, line_function_trace\n",
    "\n",
    "def line_function_data(line_function, x_values):\n",
    "    y_values = list(map(lambda x: line_function(x), x_values))\n",
    "    return {'x': x_values, 'y': y_values}\n",
    "\n",
    "def error(point, regression_formula):\n",
    "    return point['y'] - regression_formula(x)\n",
    "\n",
    "def error_line(regression_line, point):\n",
    "    y_hat = regression_line(point['x'])\n",
    "    x_value = point['x']\n",
    "    name = 'error at ' + str(round(x_value, 1))\n",
    "    error_value = format(point['y'] - y_hat, '.1f')\n",
    "    return {'x': [x_value, x_value], 'y': [point['y'], y_hat], 'mode': 'lines+text', 'marker': {'color': 'red'}, 'name': name, 'text': [error_value], 'textposition':'right'}\n",
    "    \n",
    "def error_lines(regression_line, points):\n",
    "    return list(map(lambda point: error_line(regression_line, point), points))\n",
    "\n",
    "scatter = trace(shows, mode = 'markers')\n",
    "x_values = list(map(lambda show: show['x'], shows))\n",
    "regression_plotted = line_function_trace(regression_formula, x_values)\n",
    "errors = error_lines(regression_formula, shows)\n",
    "\n",
    "plot([scatter, regression_plotted, *errors])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From there, we calculate the `residual sum of squared errors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10852.689999999993"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def regression_formula(x):     \n",
    "    return 1.417*x + 70\n",
    "\n",
    "def y(x, points):\n",
    "    point_at_x = list(filter(lambda point: point['x'] == x,points))[0]\n",
    "    return point_at_x['y']\n",
    "\n",
    "# calculate the squared error at a given value of x\n",
    "def squared_error(x, movies):\n",
    "    return (y(x, movies) - regression_formula(x))**2\n",
    "\n",
    "def sum_of_squared_errors(points):\n",
    "    squared_errors = list(map(lambda point: squared_error(point['x'], points), points))\n",
    "    return sum(squared_errors)\n",
    "\n",
    "sum_of_squared_errors(shows) # 9166.69"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, 9166.69.  Is that a good number? Who knows. Let's get a sense of this by plugging in different numbers for *b* and seeing what happens to the residual sum of squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| b        | residual sum of squared           | \n",
    "| ------------- |:-------------:| \n",
    "| 100      |9166| \n",
    "| 110      |9804 | \n",
    "| 90      |9128 | \n",
    "|80 | 9691"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now notice that while keeping our value of $m$ at 1.83, we can move towards a smaller residual sum of squares (RSS) by changing our value of $b$.  Setting $b$ to 110 produced a higher error, than at 100, so we tried moving in the other direction.  We kept moving our $b$ value lower until we set $b$ = 80, at which point our error increased from the value at 90.  So, we know that a value of $b$ between 80 and 90 produces the smallest RSS, when we set $m$ = 1.417. \n",
    "\n",
    "So our RSS is a function of how we change the $b$ value, and as we'll see, how we change the $m$ value.  This changing output of RSS based on a changing input of different regression lines is called our cost function.  You can see that if we plot our cost function as RSS with changing values of $b$, we get the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./cost-curve-plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you can see visually from this that when $b = 80$ RSS is the lowest. So we start at value 100, and we can move back and forth until we get to around 80.\n",
    "\n",
    "This technique of adjusting our values to minimize move towards a minimum value is called *gradient descent*.  Here, we *descend* along a cost curve.  When the value of our RSS no longer decreases as we change our variable, we stop.\n",
    "\n",
    "### Summary\n",
    "\n",
    "So our technique from the top of this lesson holds true: \n",
    "\n",
    "* Adjust $b$ and $m$, as these are the only things that can vary in a single-variable regression line.\n",
    "* After each adjustment calculate the average squared error \n",
    "* The regression line (that is, the values of $b$ and $m$) that produces the smallest residual sum of squares for our data is the best fit line."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
